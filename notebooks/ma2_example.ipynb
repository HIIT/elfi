{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "# Assuming we are in the notebook directory add this so that we can import the library\n",
    "sys.path.append('..')\n",
    "\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "matplotlib.style.use('ggplot')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference with ELFI: the MA(2) model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 2nd order moving average model, MA(2), is a common model used in univariate time analysis. Assuming zero mean it can be written as\n",
    "\n",
    "$$\n",
    "y_t = w_t + \\theta_1 w_{t-1} + \\theta_2 w_{t-2},\n",
    "$$\n",
    "\n",
    "where $\\theta_1, \\theta_2 \\in \\mathbb{R}$ and $(w_k)_{k\\in \\mathbb{Z}} \\sim N(0,1)$ represents an independent and identically distributed sequence of white noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The observed data and the inference problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, our task is to infer the parameters $\\theta_1, \\theta_2$ given a sequence of 100 observations $y$ that originate from an MA(2) process. Let's define this MA(2) simulator as a Python function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def MA2(t1, t2, batch_size=1, random_state=None):\n",
    "    n_obs = 100\n",
    "    if random_state is None:\n",
    "        random_state = np.random.RandomState()\n",
    "    w = random_state.randn(batch_size, n_obs+2) # i.i.d. sequence ~ N(0,1)\n",
    "    y = w[:,2:] + t1 * w[:,1:-1] + t2 * w[:,:-2]\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In ELFI, all non-constant data is in at least 2-dimensional NumPy arrays with different observations on the 0-axis. **Important**: in order to guarantee a consistent state of pseudo-random number generation, the simulator must have `prng` as a keyword argument for reading in a `numpy.RandomState` object. Additionally, a simulator that supports vectorized operations should accept the keyword argument `n_sim` that defines how many sequences should be returned.\n",
    "\n",
    "Let's now use this simulator to create the observations with true parameter values $\\theta_1=0.6, \\theta_2=0.2$ (from now on these are considered unknown):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# true parameters\n",
    "t1_0 = 0.6\n",
    "t2_0 = 0.2\n",
    "\n",
    "# Set up observed data y\n",
    "y = MA2(t1_0, t2_0)\n",
    "\n",
    "# Plot the observed sequence\n",
    "plt.figure(figsize=(11, 6));\n",
    "plt.plot(y.flatten());\n",
    "\n",
    "# To illustrate the stochasticity, let's plot a couple of more observations with the same true parameters:\n",
    "plt.plot(MA2(t1_0, t2_0).flatten());\n",
    "plt.plot(MA2(t1_0, t2_0).flatten());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approximate Bayesian Computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The figure above illustrates the difficulty of inferring $\\theta_1, \\theta_2$. One way to approach this kind of problems is Approximate Bayesian Computation (ABC), which is based on the intuition that similar data is likely to have been produced by similar parameters. Although the idea may appear inapplicable for the task at hand, it works when a large number of samples can be used. For more information about ABC, please see e.g. \n",
    "\n",
    "* [Marin, J.-M., Pudlo, P., Robert, C. P., and Ryder, R. J. (2012). Approximate Bayesian computational\n",
    "methods. *Statistics and Computing*, 22(6):1167â€“1180.](http://link.springer.com/article/10.1007/s11222-011-9288-2)\n",
    "* [Lintusaari, J., Gutmann, M. U., Dutta, R., Kaski, S., and Corander, J. (2016). Fundamentals and recent\n",
    "developments in approximate Bayesian computation. *Systematic Biology*, doi: 10.1093/sysbio/syw077.](http://sysbio.oxfordjournals.org/content/early/2016/09/07/sysbio.syw077.full.pdf)\n",
    "* https://en.wikipedia.org/wiki/Approximate_Bayesian_computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the inference problem in ELFI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In ELFI, the inference problem is described in the form of a directed acyclic graph ([DAG](https://en.wikipedia.org/wiki/Directed_acyclic_graph)), which associates each node with its parent nodes. This provides an intuitive means to describe complex dependencies that are automatically fulfilled by the inference engine. So let's build such a model.\n",
    "\n",
    "As is usual in Bayesian statistical inference, we need to define *prior* distributions for the unknown parameters $\\theta_1, \\theta_2$. In ELFI the priors can be any of the continuous and discrete distributions available in `scipy.stats` (for custom priors, see [below](#custom_prior)). For simplicity, let's start by assuming that both parameters follow `Uniform(0, 2)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import elfi\n",
    "\n",
    "# a node is defined by giving it a name, a distribution from scipy.stats and its parents (here constants 0 and 2)\n",
    "t1 = elfi.Prior('t1', scipy.stats.uniform, 0, 2)\n",
    "\n",
    "# ELFI also supports giving the scipy.stats distributions as strings\n",
    "t2 = elfi.Prior('t2', 'uniform', 0, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define the *Simulator* node by giving it the `MA2` function, and the priors as its parents. As this node can be compared with observations, we give them as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Y = elfi.Simulator('MA2', MA2, t1, t2, observed=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But how does one compare the simulations with the observed sequence? As was evident from the plot of just a few observed sequences, a direct pointwise comparison is unproductive. Indeed, the comparison of simulated sequences is often the most difficult (and arbitrary) part of ABC. Typically one chooses one or more *ad hoc* summary statistics and then calculates the discrepancy between those.\n",
    "\n",
    "Here, we will apply the intuition arising from the definition of the MA(2) process, and use the autocovariances with lags 1 and 2 as the summary statistics, and evaluate the discrepancy with the common Euclidean L2-distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def autocov(x, lag=1):\n",
    "    mu = np.mean(x, axis=1, keepdims=True)\n",
    "    C = np.mean(x[:,lag:] * x[:,:-lag], axis=1, keepdims=True) - mu**2\n",
    "    return C\n",
    "\n",
    "def distance(x, y):\n",
    "    d = np.linalg.norm( np.array(x) - np.array(y), ord=2, axis=0)\n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As is familiar by now, a `Summary` node is defined by giving the autocovariance function and the simulated data (which includes the observed as well):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "S1 = elfi.Summary('S1', autocov, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The node syntax requires a function, so defining the second summary statistic as autocovariance with lag 2 is a bit more involved. Instead a writing a complete function from scratch, we can create a *closure* that fixes the keyword argument `lag`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "# this is a more efficient way to say `lambda x: autocov(x, lag=2)`\n",
    "autocov2 = partial(autocov, lag=2)\n",
    "\n",
    "S2 = elfi.Summary('S2', autocov2, Y)\n",
    "\n",
    "# Finish the model with the final node that calculates the squared distance (S1_sim-S1_obs)**2 + (S2_sim-S2_obs)**2\n",
    "d = elfi.Discrepancy('d', distance, S1, S2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the inference model is defined, ELFI can visualize the DAG:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "elfi.draw_model(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='custom_prior'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining custom priors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the above definition is perfectly valid, having both prior distributions as `Uniform(0,2)` is bad, as the case becomes unidentifiable (i.e. the solution is not unique). To avoid this, *Marin et al. (2012)* defined the priors such that $-2<\\theta_1<2$ with $\\theta_1+\\theta_2>-1$ and $\\theta_1-\\theta_2<1$ i.e. the parameters are sampled from a triangle (see below).\n",
    "\n",
    "In ELFI, custom distributions can be defined as static classes (i.e. they only have static methods) that behave in a way similar to distributions from scipy.stats. Alternatively they can inherit `elfi.Distribution`. In this case we only need these for sampling, so implementing the static class with a rvs method suffices. As was in the context of simulators, it is important to accept the keyword argument random_state, which is needed for ELFI's internal book-keeping of pseudo-random number generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define prior for t1 as in Marin et al., 2012 with t1 in range [-b, b]\n",
    "class MarinPrior_t1():\n",
    "    def rvs(b, size=1, random_state=None):\n",
    "        u = scipy.stats.uniform.rvs(loc=0, scale=1, size=size, random_state=random_state)\n",
    "        t1 = np.where(u<0.5, np.sqrt(2.*u)*b-b, -np.sqrt(2.*(1.-u))*b+b)\n",
    "        return t1\n",
    "\n",
    "# define prior for t2 conditionally on t1 as in Marin et al., 2012, in range [-a, a]\n",
    "class MarinPrior_t2(elfi.Distribution):\n",
    "    def rvs(t1, a, size=1, random_state=None):\n",
    "        locs = np.maximum(-a-t1, t1-a)\n",
    "        scales = a - locs\n",
    "        t2 = scipy.stats.uniform.rvs(loc=locs, scale=scales, size=size, random_state=random_state)\n",
    "        return t2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These indeed sample from a triangle:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t1_1000 = MarinPrior_t1.rvs(2, 1000)\n",
    "t2_1000 = MarinPrior_t2.rvs(t1_1000, 1, 1000)\n",
    "plt.scatter(t1_1000, t2_1000, s=4, edgecolor='none');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's change the earlier priors to the new ones in the inference model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Redefine the priors\n",
    "t1.redefine(MarinPrior_t1, 2)\n",
    "t2.redefine(MarinPrior_t2, t1, 1)\n",
    "\n",
    "elfi.draw_model(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic inference with rejection sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest ABC algorithm samples parameters from their prior distributions, runs the simulator with these and compares to the observations. The samples are either accepted or rejected depending on how large the discrepancy is. The accepted samples represent samples from the posterior distribution.\n",
    "\n",
    "In ELFI, ABC methods are initialized with the discrepancy node and a list of inferred parameters (in Python lists are defined with square brackets). The optional keyword argument `batch_size` is related to parallelization (see the chapter on [parallelization](#parallelization) below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rej = elfi.Rejection(d, [t1, t2], batch_size=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the ABC method has been initialized, samples can be drawn from it. By default, rejection sampling in ELFI works in `quantile` mode i.e. a certain quantile of the samples with smallest discrepancies is accepted. The `sample` method requires the number of output samples as a parameter. Note that the simulator is then run `(N/quantile)` times.\n",
    "\n",
    "The `sample` method returns a dictionary, whose item 'samples' contains a list of the posterior numpy arrays. The dictionary also includes e.g. the item 'threshold', which is the threshold value resulting in the requested quantile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Time and run the simulator\n",
    "N = 10000\n",
    "%time result_dict = rej.sample(N, quantile=0.01)\n",
    "\n",
    "# The sample method returns a dictionary with an item `samples` that is a list of posteriors\n",
    "[t1_post, t2_post] = result_dict['samples']\n",
    "\n",
    "# print the threshold and posterior means\n",
    "print(\"Number of accepted samples {} with threshold {:.2f}\".format(len(t1_post), result_dict['threshold']))\n",
    "print(\"Posterior means: {:.2f} {:.2f}\".format(t1_post.mean(), t2_post.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rejection sampling can also be performed `threshold` based i.e. accepting all samples that result in a discrepancy below certain threshold. Note that since we require a fixed number of samples, there is no guarantee how many times the simulator will be run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%time result_dict2 = rej.sample(N, threshold=0.1)\n",
    "[t1_post2, t2_post2] = result_dict2['samples']\n",
    "print(\"Posterior means: {:.2f} {:.2f}\".format(t1_post2.mean(), t2_post2.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This needs more work... and input from YOU, yes, YOU the reader, not some Chinese person!\n",
    "\n",
    "Possibilities:\n",
    "- include plotting functionality in ABC methods, e.g. \"rej.histogram()\", \"rej.trace()\" etc.\n",
    "- associate posteriors with the original priors, e.g. \"t1.histogram()\"\n",
    "- replace the returned dictionary result_dict with an object that allows e.g. \"result.histogram()\"\n",
    "- these are not mutually exclusive\n",
    "\n",
    "Open questions:\n",
    "- What to do with results when user reruns `sample` (possibly with different settings)?\n",
    "- How to plot multidimensional inference?\n",
    "\n",
    "Feedback and suggestions welcome!\n",
    "\n",
    "Currently plotting has to be done \"manually\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=2, figsize=(14,5));\n",
    "ax[0].hist(t1_post, bins=20);\n",
    "ax[0].set_title(\"Posterior for t1\");\n",
    "ax[1].hist(t2_post, bins=20);\n",
    "ax[1].set_title(\"Posterior for t2\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# A 2D visualization may be more appropriate\n",
    "plt.scatter(t1_post, t2_post, s=1, edgecolor='none');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='parallelization'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatic parallelization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Behind the scenes, ELFI uses the [Dask](http://dask.pydata.org/) library to automatically parallelize (most of) the computational inference. The parallelization can be tuned with the keyword argument `batch_size` for the Rejection class, which tells how many \"runs\"$^1$ should be sent to each available computational unit at a time. There is some overhead involved in the parallelization, so batches should be large, but not too large to eat all your memory.\n",
    "\n",
    "$^1$To be more accurate, the MA2 model as well as the other functions in this example are vectorized, and the simulator is actually called just a few times with the keyword argument `n_sim` set to `batch_size`. This results in more efficient usage of numpy operations. Whenever possible, functions should be written in a form that allows vectorization.\n",
    "\n",
    "The underlying Dask graph for our inference can be visualized (partly):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from dask.dot import dot_graph\n",
    "# dot_graph(d[0:4].dask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential Monte Carlo ABC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rejection sampling is quite inefficient, as it does not learn from its history. The sequential Monte Carlo (SMC) ABC algorithm does just that by applying importance sampling: samples are *weighed* according to the resulting discrepancies and the next *population* of samples is drawn near to the previous using the weights as probabilities. \n",
    "\n",
    "For evaluating the weights, SMC ABC needs to have probability density functions for the priors. In our MA2 example the second prior is conditional on the first, which complicates matters a bit. Let's modify the prior distribution classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define prior for t1 as in Marin et al., 2012 with t1 in range [-b, b]\n",
    "class MarinPrior_t1(elfi.Distribution):\n",
    "    def rvs(self, b, size=1, random_state=None):\n",
    "        u = scipy.stats.uniform.rvs(loc=0, scale=1, size=size, random_state=random_state)\n",
    "        t1 = np.where(u<0.5, np.sqrt(2.*u)*b-b, -np.sqrt(2.*(1.-u))*b+b)\n",
    "        return t1\n",
    "    \n",
    "    def pdf(self, t1, b):\n",
    "        p = 1./b - np.abs(t1) / (b*b)\n",
    "        p = np.where(p < 0., 0., p)  # disallow values outside of [-b, b] (affects weights only)\n",
    "        return p\n",
    "\n",
    "# define prior for t2 conditionally on t1 as in Marin et al., 2012, in range [-a, a]\n",
    "class MarinPrior_t2(elfi.Distribution):\n",
    "    def rvs(self, t1, a, size=1, random_state=None):\n",
    "        locs = np.maximum(-a-t1, t1-a)\n",
    "        scales = a - locs\n",
    "        t2 = scipy.stats.uniform.rvs(loc=locs, scale=scales, size=size, random_state=random_state)\n",
    "        return t2\n",
    "    \n",
    "    def pdf(self, t2, _, a, all_samples):\n",
    "        t1 = all_samples[0]\n",
    "        locs = np.maximum(-a-t1, t1-a)\n",
    "        scales = a - locs\n",
    "        p = scipy.stats.uniform.pdf(t2, loc=locs, scale=scales)\n",
    "        p = np.where(scales>0., p, 0.)  # disallow values outside of [-a, a] (affects weights only)\n",
    "        return p\n",
    "    \n",
    "# Redefine the priors\n",
    "t1.redefine(MarinPrior_t1, 2)\n",
    "t2.redefine(MarinPrior_t2, t1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In ELFI, one can setup a SMC ABC sampler just like the Rejection sampler:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# smc = elfi.SMC(d, [t1, t2], batch_size=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For sampling, one has to define the number of output samples, the number of populations and a *schedule* i.e. a list of quantiles to use for each population. In essence, a population is just refined rejection sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# N = 10000\n",
    "# n_populations = 3\n",
    "# schedule = [0.1, 0.05, 0.01]\n",
    "# %time result_dict = smc.sample(N, n_populations, schedule)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what we got:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#fig, ax = plt.subplots(ncols=n_populations, sharex=True, sharey=True, figsize=(16,6))\n",
    "#for ii in range(n_populations-1):\n",
    "#    data = result_dict['samples_history'][ii]\n",
    "#    print(\"Posterior means for population {}: t1: {:.2f} t2: {:.2f}\".format(ii+1, data[0].mean(), data[1].mean()))\n",
    "#    ax[ii].scatter(data[0], data[1], s=1, edgecolor='none');\n",
    "#    ax[ii].set_title(\"Population {}\".format(ii+1));\n",
    "#data = result_dict['samples']\n",
    "#print(\"Posterior means for population {}: t1: {:.2f} t2: {:.2f}\".format(n_populations, data[0].mean(), data[1].mean()))\n",
    "#ax[n_populations-1].scatter(data[0], data[1], s=1, edgecolor='none');\n",
    "#ax[n_populations-1].set_title(\"Population {}\".format(n_populations));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen that the populations iteratively concentrate more and more around the true parameter values.\n",
    "\n",
    "Note that for the later populations some of the samples lie outside allowed region. This is due to the SMC algorithm sampling near previous samples, with *near* meaning a Gaussian distribution centered around previous samples with variance as twice the weighted empirical variance. However, the outliers carry zero weight, as was defined in the prior pdfs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian Optimization for Likelihood-Free Inference (BOLFI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice inference problems often have a more complicated and computationally heavy simulator than the function `MA2` here, and one simply cannot run it for millions of times. The [BOLFI](https://arxiv.org/abs/1501.03291) framework is likely to prove useful in such situation: a statistical model (e.g. [Gaussian process](https://en.wikipedia.org/wiki/Gaussian_process)) is created for the discrepancy, and its minimum is inferred with [Bayesian optimization](https://en.wikipedia.org/wiki/Bayesian_optimization). This approach typically reduces the number of required simulator calls by several orders of magnitude.\n",
    "\n",
    "As BOLFI is more advanced inference method, its interface is also a bit more involved. But not much: Using the same graphical model as earlier, the inference begins by defining a Gaussian process model, for which we use the [GPy](https://sheffieldml.github.io/GPy/) library. We are inferring 2 parameters with the same bounds as earlier, and we have to give these conditions to the surrogate model i.e. the Gaussian process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gp_model = elfi.GPyModel(input_dim=2, bounds=((-2,2), (-1,1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having defined the surrogate model, we can instantiate the BOLFI framework object in a somewhat similar way as earlier, except that we now additionally define the surrogate model and the number of samples to take from it (this is the number of simulator calls): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bolfi = elfi.BOLFI(d, [t1, t2], batch_size=20, n_surrogate_samples=150, model=gp_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The BOLFI class can now try to `infer` the posterior distribution of the parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "post = bolfi.infer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get estimates for *maximum a posteriori* and *maximum likelihood* easily:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "post.MAP, post.ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also plot the Gaussian process using GPy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "bolfi.model.gp.plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(bolfi.model.gp.X[:,0], bolfi.model.gp.Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... or we can visualize the posterior directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x, y = np.meshgrid(np.linspace(-2, 2), np.linspace(-1, 1))\n",
    "z = np.empty_like(x)\n",
    "for ii in range(len(x)):\n",
    "    for jj in range(len(x)):\n",
    "        z[ii, jj] = post.pdf(np.array([x[ii, jj], y[ii, jj]]))\n",
    "plt.contour(x, y, z);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving samples for later use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving performed computations is often a good idea, especially if the simulator is expensive to call. ELFI supports persistence of results in multiple formats. The user simply has to define this with the `store` keyword argument, which can be given to all nodes in the graphical model upon instantiation.\n",
    "\n",
    "To cache the prior `t1` in memory (affects only `t1`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t1_cache = elfi.Prior('t1_cache', 'uniform', 0, 1, store=\"cache\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To store directly in a Numpy array, which can afterwards be processed using any Numpy compatible library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "max_samples = 10000000\n",
    "t1_array = np.empty((max_samples, 1))  # make sure you allocate enough memory\n",
    "t1_arr = elfi.Prior('t1_arr', 'uniform', 0, 1, store=t1_array)\n",
    "\n",
    "# Save in binary Numpy format:\n",
    "# np.save('t1.npy', t1_array)\n",
    "\n",
    "# Save in ascii csv format:\n",
    "# np.savetxt('t1.csv', t1_array, delimiter=', ')\n",
    "\n",
    "# Save in HDF5 format:\n",
    "# import h5py\n",
    "# h5file = h5py.File('ma2.hdf', 'w')\n",
    "# h5file.create_dataset('t1', data=t1_array)\n",
    "# h5file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numpy arrays can also be used indirectly via a `LocalDataStore` object:\n",
    "\n",
    "**...... Is there some benefit from using this?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "object_store = elfi.LocalDataStore(t1_array)\n",
    "t1_obj = elfi.Prior('t1_obj', 'uniform', 0, 1, store=object_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A NoSQL database is also supported, in which case the results are saved to collection matching node name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nosql_store = elfi.UnQLiteStore()  # accepts filename to save to disk\n",
    "Y_sql = elfi.Simulator('MA2_sql', MA2, t1, t2, observed=y, store=nosql_store)\n",
    "\n",
    "# afterwards get data:\n",
    "# Y_array = nosql_store.get('Y', slice(0,1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "conda-env-abc4py-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
